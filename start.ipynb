{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Начало обучение NLP  с помощью пакета spaCY\n",
    "\n",
    "Старт: 01.10.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конвеер\n",
    "### Токенезация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет', ',', 'меня', 'зовут', 'Денис']\n"
     ]
    }
   ],
   "source": [
    "# демонстрация токенезации\n",
    "\n",
    "doc = nlp('Привет, меня зовут Денис')\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я я\n",
      "вижу видеть\n",
      "красивейший красивый\n",
      "закат закат\n",
      "и и\n",
      "балдею балдеть\n",
      "и и\n",
      "влюбляюсь влюбляться\n",
      "в в\n",
      "Питер питер\n"
     ]
    }
   ],
   "source": [
    "# пример лемматизации\n",
    "\n",
    "doc = nlp(u'Я вижу красивейший закат и балдею и влюбляюсь в Питер')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пимер: бот для бронирования авиобилетов. Нужно счиатать строку и добавить в исключение названия городов. Допустим слово\n",
    "\"Питер\" должно при леммизации сделаться \"Санкт-Петербург\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Я', 'хочу', 'купить', 'билет', 'в', 'Питер']\n",
      "['token:Я lemma:я', 'token:хочу lemma:хотеть', 'token:купить lemma:купить', 'token:билет lemma:билет', 'token:в lemma:в', 'token:Питер lemma:Санкт-Петербург']\n",
      "['я', 'хотеть', 'купить', 'билет', 'в', 'Санкт-Петербург']\n"
     ]
    }
   ],
   "source": [
    "input_str = u'Я хочу купить билет в Питер'\n",
    "doc = nlp(input_str )\n",
    "\n",
    "print([w.text for w in doc]) # ['Я', 'хочу', 'купить', 'билет', 'в', 'Питер'] - плохо\n",
    "\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"TEXT\": \"Питер\"}]], {\"LEMMA\": \"Санкт-Петербург\"})\n",
    "\n",
    "doc_ru = nlp(input_str)    \n",
    "print(['token:%s lemma:%s' % (t.text, t.lemma_) for t in doc])\n",
    "\n",
    "\n",
    "print([w.lemma_ for w in nlp(input_str)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частеречная разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I PRON PRP\n",
      "have have AUX VBP\n",
      "flown fly VERB VBN\n",
      "to to ADP IN\n",
      "LA LA PROPN NNP\n",
      ". . PUNCT .\n",
      "Now now ADV RB\n",
      "I I PRON PRP\n",
      "am be AUX VBP\n",
      "flying fly VERB VBG\n",
      "to to ADP IN\n",
      "Frisco Frisco PROPN NNP\n",
      "['flying']\n"
     ]
    }
   ],
   "source": [
    "# частеречная разметка\n",
    "nlp_en = sp.load('en_core_web_sm')\n",
    "doc = nlp_en(u'I have flown to LA. Now I am flying to Frisco')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_)\n",
    "\n",
    "print([w.text for w in doc if w.tag_=='VBG' or w.pos_ =='VB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посмотреть какие отноешния spacy установил между словами при частиречной разметке. Посмотрим метки синтаксичечкой зависимости.\n",
    "\n",
    "ROOT - смысловой глагол в предложении \n",
    "nsubj - подлежащие \n",
    "\n",
    "Эти два тега есть "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input str: \n",
      "I have flown to LA. Now I am flying to Frisco\n",
      "--------------------\n",
      "I PRON nsubj\n",
      "have AUX aux\n",
      "flown VERB ROOT\n",
      "to ADP prep\n",
      "LA PROPN pobj\n",
      ". PUNCT punct\n",
      "Now ADV advmod\n",
      "I PRON nsubj\n",
      "am AUX aux\n",
      "flying VERB ROOT\n",
      "to ADP prep\n",
      "Frisco PROPN pobj\n"
     ]
    }
   ],
   "source": [
    "print('input str: ')\n",
    "print(doc)\n",
    "print(5*'----')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ce3869865918335a5f7f8bf564fd081cbfe04ed0827b527a250169e23ceeace"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
