{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с объектами-контейнерами и настройка spaCy\n",
    "под свои нужды\n",
    "========================\n",
    "05.10.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные объекты, из которых состоит API spaCy, можно разделить на две категории: контейнеры (например, объекты Token и Doc) и компоненты конвейеров обработки (например, средства частеречной разметки и распознавания именованных сущностей). В этой главе продолжим изучение объектов-контейнеров: благодаря им и их методам можно получать доступ к лингвистическим меткам, которые библиотека spaCy присваивает всем токенам в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp\n",
    "nlp = sp.load('ru_core_news_sm')\n",
    "input_1 = 'Здравствуйте. У меня болит горло..'\n",
    "input_2 = 'Привет, как дела?'\n",
    "input_3 = 'Где кошка'\n",
    "input_4 = 'Хочу купить булочку'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./doc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Здравствуйте, ., У, меня, болит, горло, ..]\n",
      "[('Здравствуйте', 'ROOT', 'Здравствуйте'), ('Здравствуйте', 'punct', '.'), ('меня', 'case', 'У'), ('болит', 'obl', 'меня'), ('болит', 'ROOT', 'болит'), ('болит', 'nsubj', 'горло'), ('болит', 'punct', '..')]\n"
     ]
    }
   ],
   "source": [
    "from traceback import print_tb\n",
    "\n",
    "\n",
    "doc = nlp(input_1)\n",
    "\n",
    "doc_list = [doc[i] for i in range(len(doc))]   # по индексам\n",
    "print(doc_list)\n",
    "\n",
    "# можно обращаться к токенам\n",
    "\n",
    "print([(token.head.text, token.dep_, token.text) for token in doc])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть нам нужно найти левосторонний дочерний элемент токена в дереве синтаксических зависимостей предложения. Данная операция\n",
    "позволяет найти прилагательные (при их наличии) для заданного\n",
    "существительного. Этоможет понадобиться, если нужно узнать, какие\n",
    "прилагательные могут модифицировать заданное существительное.\n",
    "\n",
    "Можно также использовать атрибут Token.rights для получения правосторонних синтаксических дочерних элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "болит\n",
      "[меня]\n"
     ]
    }
   ],
   "source": [
    "print(doc[4]) # слово\n",
    "print([w for w in doc[4].lefts]) # левостороннее дочернее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Контейнер doc.sents\n",
    "doc.sents объекта Doc текст можно разделить на\n",
    "отдельные предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Здравствуйте. У меня болит горло..\n",
      "[Здравствуйте, .]\n",
      "[У, меня, болит, горло, ..]\n"
     ]
    }
   ],
   "source": [
    "print(input_1)\n",
    "doc = nlp(input_1)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print([sent[i] for i in range(len(sent))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В то же время можно ссылаться на токены в состоящем из множества\n",
    "предложений тексте с помощью глобальных индексов уровня документа, как показано вот здесь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Здравствуйте, ., У, меня, болит, горло, ..]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc[i] for i in range(len(doc))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second sentence begins with a pronoun.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Здравствуйте Денис. Я могу предложить вам чай')\n",
    "\n",
    "\n",
    "for i,sent in enumerate(doc.sents):\n",
    "    if i==1 and sent[0].pos_== 'PRON':\n",
    "        print('The second sentence begins with a pronoun.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предлодений с глаглом на конце: 2\n"
     ]
    }
   ],
   "source": [
    "# сколько предложений в текстве оканчивается глаголом?\n",
    "doc = nlp(u'Здравствуйте Денис.я хочу пить. Ты не хочешь. Ну и бог с тобой')\n",
    "\n",
    "counter = 0\n",
    "for sent in doc.sents:\n",
    "    if sent[len(sent)-2].pos_ == 'VERB':\n",
    "        counter+=1\n",
    "\n",
    "print('Предлодений с глаглом на конце:', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Контейнер doc.noun_chunks\n",
    "\n",
    "С помощью свойства doc.noun_chunks объекта Doc можно пройти по\n",
    "именным фрагментам. Именной фрагмент (noun chunk) — это фраза,\n",
    "главнымэлементомкоторой является существительное. (НЕ РАБОТАЕТ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NOUN', 'Собака'), ('VERB', 'роет'), ('ADJ', 'глубокую'), ('NOUN', 'яму'), ('PUNCT', '.'), ('NUM', 'Рот'), ('NOUN', 'собаки'), ('ADP', 'в'), ('NOUN', 'грязи')]\n",
      "Собака\n",
      "глубокую яму\n",
      "собаки\n",
      "грязи\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# сделаем это же вручную\n",
    "doc = nlp('Собака роет глубокую яму. Рот собаки в грязи')\n",
    "print([(token.pos_, token.text) for token in doc])\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_=='NOUN':  # ищем существительное\n",
    "        chunk = ''\n",
    "        for w in token.children:\n",
    "            if w.pos_ == 'DET' or w.pos_ == 'ADJ':\n",
    "                chunk = chunk + w.text + ' '\n",
    "        chunk = chunk + token.text     \n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание:используемые длямодификациисуществительных\n",
    "слова (определителии прилагательные)всегдаявляютсялевосторонними\n",
    "дочерними элементами для существительного. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Собака\n",
      "глубокую яму\n",
      "Рот собаки\n",
      "в грязи\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for token in doc:\n",
    "  if token.pos_=='NOUN':  # ищем существительное\n",
    "        chunk = ''\n",
    "        for w in token.lefts:\n",
    "            chunk = chunk + w.text + ' '\n",
    "        chunk = chunk + token.text     \n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Объект Span\n",
    "\n",
    "Объект Span (от англ. span — «интервал») представляет собой часть\n",
    "объекта Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Арбуз будешь?"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('Привет, Арбуз будешь?')\n",
    "doc[2:5] # срезы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Я, еду, из, Ростова, в, Санк, -, Петербург, на, поезде]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# span.merge()\n",
    "doc = nlp('Я еду из Ростова в Санк-Петербург на поезде')\n",
    "\n",
    "[doc[i] for i in range(len(doc))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----later merge -----\n",
      "Я PRON nsubj еду\n",
      "еду VERB ROOT еду\n",
      "из ADP case Ростова\n",
      "Ростова PROPN obl еду\n",
      "в ADP case Санк\n",
      "Санк PROPN obl еду\n",
      "- PROPN obl еду\n",
      "Петербург PROPN obl еду\n",
      "на ADP case поезде\n",
      "поезде NOUN obl еду\n",
      "------- what merge:\n",
      "Санк-Петербург\n",
      "-------\n",
      "-----after merge: Санкт-Петербург в одну лемму-----\n",
      "Я PRON nsubj еду\n",
      "еду VERB ROOT еду\n",
      "из ADP case Ростова\n",
      "Ростова PROPN obl еду\n",
      "в ADP case Санк-Петербург\n",
      "Санк-Петербург PROPN obl еду\n",
      "на ADP case поезде\n",
      "поезде NOUN obl еду\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(5*'-' + 'later merge ' + 5*'-')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "\n",
    "span = doc[5:8]\n",
    "print('------- what merge:')\n",
    "print(span)\n",
    "print('-------')\n",
    "lem_id = doc.vocab.strings[span.text]\n",
    "\n",
    "with doc.retokenize() as retokenizer:\n",
    "    retokenizer.merge(span)\n",
    "\n",
    "\n",
    "print(5*'-' + 'after merge: Санкт-Петербург в одну лемму' + 5*'-')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Настройка конвейера обработки текста под свои нужды\n",
    "\n",
    "Из предыдущих разделов вы узнали, что объекты-контейнеры spaCy\n",
    "соответствуют различным лингвистическим единицам (например,\n",
    "целому тексту или отдельному токену), что позволяет извлекать линг-\n",
    "вистические признаки этих единиц. Теперь же рассмотрим объекты\n",
    "API spaCy, предназначенные для создания контейнеров и заполнения\n",
    "их подходящими данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names #Посмотреть доступные для объекта nlp компоненты конвейера можно с помощью команды:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Отключение компонентов конвейера\n",
    "\n",
    "spaCy позволяет выбирать загружаемые компоненты конвейера и отключать те, которые не нужны. Это можно сделать при создании объекта nlp, задав параметр disable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load('ru_core_news_sm', disable=['parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данном случае мы создадим конвейер обработки без утилиты разбора зависимостей. При вызове такого экземпляра nlp для конкретного\n",
    "текста токены в этом тексте не получат метки зависимостей. Вышесказанное наглядно иллюстрирует следующий пример:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я PRON \n",
      "хочу VERB \n",
      "скушать VERB \n",
      "яблочко NOUN \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Я хочу скушать яблочко')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "#Однако метки зависимостей выведены не были."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пошаговая загрузка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью метода spacy.load(), загружающего модель, за один раз\n",
    "можно произвести несколько операций. Например, при выполнении\n",
    "оператора:\n",
    "nlp = spacy.load('en')\n",
    "библиотека spaCy осуществляет такие действия.\n",
    "1. По названию загружаемой модели определяет, какой экземпляр\n",
    "класса Language следует инициализировать. В данном примере spaCy\n",
    "создает экземпляр подкласса English, включающий общий словарь\n",
    "и прочие языковые данные.\n",
    "2. Проходит в цикле по названиям компонентов конвейера обработки,\n",
    "создает соответствующие компоненты и добавляет их в конвейер\n",
    "обработки.\n",
    "3. Загружает данные модели с диска, делая их доступными для экземпляра класса Language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод spacy.load() скрывает эти нюансы реализации, в большинстве\n",
    "случаев экономя ваши силы и время. Но иногда для более тонкой\n",
    "настройки процесса эти шаги приходится реализовывать явным образом: подобное, например, может понадобиться для подключения\n",
    "к конвейеру обработки пользовательского компонента для вывода\n",
    "какой-либо информации об объекте Doc (числа токенов, наличия/отсутствия определенных частей речи и т. д.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем шире возможности настройки, тембольше информации требуется\n",
    "указать. Чтобы узнать путь к пакету модели, лучше не использовать\n",
    "сокращенное наименование, а получить настоящее название модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru_core_news_sm\n"
     ]
    }
   ],
   "source": [
    "print(nlp.meta['lang'] + '_' + nlp.meta['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru_core_news_sm-3.4.0\n"
     ]
    }
   ],
   "source": [
    "print(nlp.meta['lang'] + '_' + nlp.meta['name'] + '-' + nlp.\n",
    "meta['version'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'morphologizer', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.meta['pipeline'] #список компонентов конвейера, используемых с моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.tagger.Tagger object at 0x0000020EA25AD820> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-de38e6a998ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcomponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_data_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stron\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    771\u001b[0m             \u001b[0mbad_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE966\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbad_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.tagger.Tagger object at 0x0000020EA25AD820> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ce3869865918335a5f7f8bf564fd081cbfe04ed0827b527a250169e23ceeace"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
