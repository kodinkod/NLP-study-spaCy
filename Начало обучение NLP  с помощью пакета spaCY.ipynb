{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Начало обучение NLP  с помощью пакета spaCY\n",
    "###### часть 1 Глава 2. Конвейер обработки текста\n",
    "Старт: 01.10.2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = sp.load('ru_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конвеер\n",
    "### Токенезация\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Привет', ',', 'меня', 'зовут', 'Денис']\n"
     ]
    }
   ],
   "source": [
    "# демонстрация токенезации\n",
    "\n",
    "doc = nlp('Привет, меня зовут Денис')\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я я\n",
      "вижу видеть\n",
      "красивейший красивый\n",
      "закат закат\n",
      "и и\n",
      "балдею балдеть\n",
      "и и\n",
      "влюбляюсь влюбляться\n",
      "в в\n",
      "Питер питер\n"
     ]
    }
   ],
   "source": [
    "# пример лемматизации\n",
    "\n",
    "doc = nlp(u'Я вижу красивейший закат и балдею и влюбляюсь в Питер')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пимер: бот для бронирования авиобилетов. Нужно счиатать строку и добавить в исключение названия городов. Допустим слово\n",
    "\"Питер\" должно при леммизации сделаться \"Санкт-Петербург\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Я', 'хочу', 'купить', 'билет', 'в', 'Питер']\n",
      "['token:Я lemma:я', 'token:хочу lemma:хотеть', 'token:купить lemma:купить', 'token:билет lemma:билет', 'token:в lemma:в', 'token:Питер lemma:питер']\n",
      "['я', 'хотеть', 'купить', 'билет', 'в', 'Санкт-Петербург']\n"
     ]
    }
   ],
   "source": [
    "input_str = u'Я хочу купить билет в Питер'\n",
    "doc = nlp(input_str )\n",
    "\n",
    "print([w.text for w in doc]) # ['Я', 'хочу', 'купить', 'билет', 'в', 'Питер'] - плохо\n",
    "\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"TEXT\": \"Питер\"}]], {\"LEMMA\": \"Санкт-Петербург\"})\n",
    "\n",
    "doc_ru = nlp(input_str)    \n",
    "print(['token:%s lemma:%s' % (t.text, t.lemma_) for t in doc])\n",
    "\n",
    "\n",
    "print([w.lemma_ for w in nlp(input_str)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частеречная разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I PRON PRP\n",
      "have have AUX VBP\n",
      "flown fly VERB VBN\n",
      "to to ADP IN\n",
      "LA LA PROPN NNP\n",
      ". . PUNCT .\n",
      "Now now ADV RB\n",
      "I I PRON PRP\n",
      "am be AUX VBP\n",
      "flying fly VERB VBG\n",
      "to to ADP IN\n",
      "Frisco Frisco PROPN NNP\n",
      "['flying']\n"
     ]
    }
   ],
   "source": [
    "# частеречная разметка\n",
    "nlp_en = sp.load('en_core_web_sm')\n",
    "doc = nlp_en(u'I have flown to LA. Now I am flying to Frisco')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_)\n",
    "\n",
    "print([w.text for w in doc if w.tag_=='VBG' or w.pos_ =='VB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы посмотреть какие отноешния spacy установил между словами при частиречной разметке. Посмотрим метки синтаксичечкой зависимости.\n",
    "\n",
    "ROOT - смысловой глагол в предложении \n",
    "nsubj - подлежащие \n",
    "\n",
    "Эти два тега есть  в люьом законченном предложении, однако не все предложения на пракктике законченные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input str: \n",
      "I have flown to LA. Now I am flying to Frisco.\n",
      "--------------------\n",
      "I ROOT I\n",
      "I flat:foreign have\n",
      "I flat:foreign flown\n",
      "I flat:foreign to\n",
      "I flat:foreign LA\n",
      "I punct .\n",
      "Now ROOT Now\n",
      "Now flat:foreign I\n",
      "Now flat:foreign am\n",
      "Now flat:foreign flying\n",
      "Now flat:foreign to\n",
      "Now flat:foreign Frisco\n",
      "Now punct .\n"
     ]
    }
   ],
   "source": [
    "print('input str: ')\n",
    "print(doc)\n",
    "print(5*'----')\n",
    "for token in doc:\n",
    "    print(token.head.text, token.dep_, token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input str: \n",
      "Я хочу купить билет в Питер\n",
      "--------------------\n",
      "хочу nsubj Я\n",
      "хочу ROOT хочу\n",
      "хочу xcomp купить\n",
      "купить obj билет\n",
      "Питер case в\n",
      "купить obl Питер\n"
     ]
    }
   ],
   "source": [
    "print('input str: ')\n",
    "print(doc_ru)\n",
    "print(5*'----')\n",
    "for token in doc_ru:\n",
    "    print(token.head.text, token.dep_, token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что главная мысль предложения в метках root и obj(nobj in en);\n",
    "\n",
    "Попробуем находить слова, соответствующие этим зависимостям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flown nsubj I\n",
      "flown aux have\n",
      "flown ROOT flown\n",
      "flown prep to\n",
      "to pobj LA\n",
      "flown punct .\n",
      "flying advmod Now\n",
      "flying nsubj I\n",
      "flying aux am\n",
      "flying ROOT flying\n",
      "flying prep to\n",
      "to pobj Frisco\n",
      "flying punct .\n",
      "--------новое предложение------------\n",
      "['flown', 'LA']\n",
      "--------новое предложение------------\n",
      "['flying', 'Frisco']\n"
     ]
    }
   ],
   "source": [
    "doc_en = nlp_en(u'I have flown to LA. Now I am flying to Frisco.')\n",
    "\n",
    "for token in doc_en:\n",
    "    print(token.head.text, token.dep_, token.text)\n",
    "\n",
    "for sent in doc_en.sents:\n",
    "    print('--------новое предложение------------')\n",
    "    print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'pobj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "зовут obj Меня\n",
      "зовут ROOT зовут\n",
      "зовут nsubj Денис\n",
      "зовут punct .\n",
      "хочу nsubj Я\n",
      "хочу ROOT хочу\n",
      "хочу xcomp полететь\n",
      "Питер case в\n",
      "полететь obl Питер\n",
      "устал punct ,\n",
      "устал mark потому\n",
      "потому fixed что\n",
      "устал nsubj я\n",
      "полететь advcl устал\n",
      "болеет cc и\n",
      "болеет nsubj кот\n",
      "устал conj болеет\n",
      "хочу punct .\n",
      "улетел nsubj Кот\n",
      "улетел ROOT улетел\n",
      "Питер case в\n",
      "улетел obl Питер\n",
      "улетел punct .\n",
      "--------новое предложение------------\n",
      "['Меня', 'зовут']\n",
      "--------новое предложение------------\n",
      "['хочу', 'полететь', 'Питер']\n",
      "--------новое предложение------------\n",
      "['улетел', 'Питер']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Меня зовут Денис. Я хочу полететь в Питер, потому что я устал и кот болеет. Кот улетел в Питер.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.head.text, token.dep_, token.text)\n",
    "\n",
    "for sent in doc.sents: #doc.sents разбивает текст по предложениям\n",
    "    print('--------новое предложение------------')\n",
    "    # печатаем главный смысл\n",
    "    print([w.text for w in sent if w.dep_ == 'ROOT' or w.dep_ == 'obj' or  w.dep_ == 'obl' or  w.dep_ == 'xcomp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Распознование именовоных сущностей\n",
    "\n",
    "**Именованная сущность** (named entity) — объект, на который можно ссылаться по его собственному наименованию. Именованной сущностью может быть человек, организация, место или другая сущность. Именованные сущности играют важную роль в NLP, поскольку позволяют выяснить, о каком месте или организации говорит пользователь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA GPE\n",
      "Frisco ORG\n",
      "GPE - Геополитическая сущность\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_en(u'I have flown to LA. Now I am flying to Frisco.')\n",
    "for token in doc:\n",
    "    if token.ent_type != 0:\n",
    "        print(token.text, token.ent_type_)\n",
    "print('GPE - Геополитическая сущность')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если атрибуту ent_type токена не присвоено значение 0, значит, этот токен — именованная сущность. В таком случае выводим атрибут ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Денис PER\n",
      "Питер LOC\n",
      "Питер LOC\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Меня зовут Денис. Я хочу полететь в Питер, потому что я устал и кот болеет. Кот улетел в Питер.')\n",
    "for token in doc:\n",
    "    if token.ent_type != 0:\n",
    "        print(token.text, token.ent_type_)\n",
    "\n",
    "# PER - персона\n",
    "# LOC - локация"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ce3869865918335a5f7f8bf564fd081cbfe04ed0827b527a250169e23ceeace"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
